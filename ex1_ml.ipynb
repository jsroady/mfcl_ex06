{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (ML)\n",
    "\n",
    "\n",
    "## Topics\n",
    "\n",
    "* Classification (with a class separating hyperplane)\n",
    "* The Cost Function $J$ and its 1st Derivative (Slope)\n",
    "* Gradient Descend\n",
    "* Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "The **Formula** of a linear equation:\n",
    "\n",
    "$f(x) = y = w_1*x_1 + w_2*x_2 + b$\n",
    "\n",
    "with $b$ = intercept/y-offset and $w$ = slope.\n",
    "\n",
    "* Example: $~~~~~~~~y= -0.5x_1 + 2x_2 + 1.5$\n",
    "\n",
    "Classification is a process of finding a hyperplane that divides multiple data points into the respective class based on different features.\n",
    "\n",
    "As an example we will us the following data points and their respective classes:<br>\n",
    "$Class+: p_1 = \\langle 0.5, 3\\rangle$, $p_2 = \\langle 0.4, 7\\rangle$, and $Class-: p_3 = \\langle 0.5, 0.6\\rangle$, $p_4 = \\langle 0.2, -1\\rangle$.<br>\n",
    "Remeber that to project a hyperplane into feature space we need the y as carrier of the specific class.<br>\n",
    "Note: In python the y and therefore class information is the last number of the point array (in our example at index 2, position 3).\n",
    "\n",
    "Look at the code below and its output. The code inserts the points and the initial hyperplane in a three-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifing the points with the class included (index 2, position 3)\n",
    "p1 = [0.5, 3, 1]\n",
    "p2 = [0.4, 7, 1]\n",
    "p3 = [0.5, 0.6, -1]\n",
    "p4 = [0.2, -1, -1]\n",
    "\n",
    "# Creation of the figure and axis\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Grid configuration (evenly spaced numbers over a specified interval)\n",
    "x1 = np.linspace(0,2.5,2)\n",
    "x2 = np.linspace(0,2.5,2)\n",
    "\n",
    "# Creation of the grid and defining the initial hyperplane\n",
    "X1,X2= np.meshgrid(x1,x2)\n",
    "y = -0.5*X1 + 2*X2 + 1.5\n",
    "\n",
    "# Try commenting this out to see that, at the moment, three points lie beneath the plane (the points on the right) and one\n",
    "# point lies above the hyperplane (the point on the left)\n",
    "ax.plot_surface(X1, X2, y, color='#baffde', alpha=0.6)\n",
    "\n",
    "# Plotting the points (colours are the intended class)\n",
    "ax.scatter(0.5, 3, 1, c='#409fff',marker='o', label='class +')\n",
    "ax.scatter(0.4, 7, 1, c='#409fff',marker='o')\n",
    "\n",
    "ax.scatter(0.5, 0.6, -1, c='#f2305d',marker='^', label='class -')\n",
    "ax.scatter(0.2, -1, -1, c='#f2305d',marker='^')\n",
    "\n",
    "ax.scatter(0,0,0, c='w',marker='^')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "# Adjusting the view of the 3d graph\n",
    "# if you want to make all 3d graphs comparable, either set these values to the same ones, or comment out this line\n",
    "ax.view_init(35, 10)\n",
    "\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_zlabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This hyperplane is not ideal, we will update it further below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above linear equation:    \n",
    "$~~~~~~~~y= -0.5x_1 + 2x_2 + 1.5$<br>\n",
    "we can write a function in python that return the result of the initial hyperplane's classification (see code below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(p):\n",
    "    '''\n",
    "    This function returns the result of the initial hyperplane's classification\n",
    "    '''\n",
    "    x1, x2, y = p\n",
    "    return -0.5*x1 + 2*x2 + 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Given the same linear equation as above, $-0.5x_1 + 2x_2 + 1.5$, and the following points:<br>\n",
    "$Class+: \\langle 2, -1.5\\rangle, \\langle 1, 0.5\\rangle$ and $Class-: \\langle -0.5, 0.4\\rangle$.\n",
    "\n",
    "1. Define the points including the class label (e.g. $[x_1,x_2,label]$) and insert them into the linear equation, by calling the above $f(point)$ function for each point and printing out the results. Do not forget to answer the question below.\n",
    "2. Visualize result1, result2 and result3 and check if the classification is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 1 ###\n",
    "\n",
    "# Defining the points\n",
    "## TO-DO ##\n",
    "\n",
    "# Calling the f(p) function once for each point\n",
    "## TO-DO ##\n",
    "\n",
    "# Printing of the predicted classes\n",
    "## TO-DO ##\n",
    "\n",
    "# How would you classify each point?\n",
    "## TO-DO ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot it all in a three-dimensional space. <br> <em>Hint: If you do not know how to make plots using the package matplotlib, get inspired by looking at the code for plots in this notebook and/or the official documentation.</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 2 ###\n",
    "\n",
    "# Creation of the figure and axis\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Grid configuration (evenly spaced numbers over a specified interval)\n",
    "x1 = np.linspace(0,2.5,2)\n",
    "x2 = np.linspace(0,2.5,2)\n",
    "\n",
    "# Creation of the grid and defining the initial hyperplane\n",
    "## TO-DO ##\n",
    "\n",
    "# Plotting the points (colours are the intended class)\n",
    "## TO-DO ##\n",
    "\n",
    "plt.title(\"Initial Hyperplane\")\n",
    "ax.view_init(25, 30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the plane does not clearly separate the two classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cost Function $J$ and its 1st Derivative (Slope)\n",
    "\n",
    "**The goal of classification is to find a hyperplane that accurately predicts to which class a point belongs to.**\n",
    "\n",
    "### How to find the hyperplane\n",
    "\n",
    "To find the best fitting hyperplane we need to define a so-called cost function $J$ (also known as error function), which tells us how far off our current hyperplane is. This is done by calculating the difference between the prediction and the real value and squaring the result. By setting the first derivative of the cost function to zero and resolving it to $w$ we can update the hyperplane (i.e. weights) to improve the prediction.<br>\n",
    "\n",
    "**The goal is to find a weight vector such that the prediction error is minimal.**\n",
    "\n",
    "![image](figures/error_vis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cost function and its minimum\n",
    "    \n",
    "   $$J(w) = \\sum_{j=0}^M(y_{obs}^j-y_{pred}^j)^2~~~ or~~~ J(w)=\\sum_{j=0}^n(y^j-\\sum_k w_k*x_k^j)^2$$\n",
    "   with a partial derivative (wrt $w_i$):\n",
    "   \n",
    "   \n",
    "   $$\\frac{\\delta J(w)}{\\delta w_i}=-\\sum_{j=0}^n(y^j-\\sum_{k=0}^m w_k*x_k^j)x_i^j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em><b>We will work with the same function and points as above (Question 1).</b></em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same points as above\n",
    "points = [p1, p2, p3]\n",
    "\n",
    "# Our starting weight vector, including the bias\n",
    "w = [1.5, -0.5, 2]\n",
    "\n",
    "def costJ(w, x1, x2, y):\n",
    "    x = [1, x1, x2]\n",
    "    return (y - np.dot(w, x))**2\n",
    "\n",
    "def sum_of_costJ(w):\n",
    "    '''\n",
    "    The cost function J calculates how far off our current hyperplane is\n",
    "    '''\n",
    "    total_cost = 0\n",
    "    for point in list(points):\n",
    "        total_cost += costJ(w, point[0], point[1], point[2])\n",
    "        \n",
    "    return total_cost\n",
    "\n",
    "sum_of_costJ(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_J(w, x1, x2, y):\n",
    "    x = [1, x1, x2]\n",
    "    return np.dot((y - np.dot(w, x)), x)\n",
    "\n",
    "def slope_of_cost_function_at_w(w):\n",
    "    '''\n",
    "    To minimize the cost (error) we take the 1st derivative of J \n",
    "    '''\n",
    "    total = 0\n",
    "    for point in list(points):\n",
    "        total += derivative_J(w, point[0], point[1], point[2])\n",
    "        \n",
    "    return -total\n",
    "\n",
    "slope_of_cost_function_at_w(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have for an intial $\\vec{w}$ of $\\langle1.5, -0.5, 2\\rangle$, cost (error) of $25.8525$ and the adapted $\\vec{w}$ of $\\langle1.05, -7.775, 7.17\\rangle$ (slopes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Gradient Descend\n",
    "To minimize the error we have to adapt the weights not just one time but multiple times. To do this we need an iterative approach called gradient descend.\n",
    "\n",
    "We initialize $w$ randomly, we apply $J$ and its 1st derivative to get the slope at a particular point on the curve:\n",
    "\n",
    "![image](figures/gradient_descend.png)\n",
    "\n",
    "This point will have a sign, it will either be plus or minus:\n",
    "* if it has a plus sign, then we have to decrease the weight (we are to the right of the minimum)\n",
    "* if it has a minus sign, we have to increase the weight (we are to the left of the minimum)\n",
    "\n",
    "In order to avoid missing the minimum, we should adapt the weights slowly. To do this we have to define a learning rate $\\eta$ (usually small):\n",
    "\n",
    "$$\\Delta(w_j)=-\\eta \\frac{\\partial  J}{\\partial w_j}$$\n",
    "\n",
    "i.e. for each weight $w_j$ we determine a delta, some correction of the weight, scaled by $\\eta$\n",
    "\n",
    "* if the slope is negative, we get a plus (minus * minus = plus). The adapted weight is then: $w_j=w_j+\\Delta w_j$\n",
    "* if the slope is positive, we get a minus (minus * plus = minus). The adapted weight is then: $w_j=w_j-\\Delta w_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "In our gradient descend scenario (see output of function $slope\\_of\\_cost\\_function\\_at\\_w(w)$ the first derivative of the weight $w_1$ (weight of $x_1$) is $-7.775$ and the first derivative of the weight $w_2$ (weight of $x_2$) is $7.17$ . What does this mean for $w_1$ and $w_2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Question 2:\n",
    "## TO-DO ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descend for Hyperplane Learning (Minimizing the Error)\n",
    "\n",
    "# Defining the number of iterations it will adapt the weights\n",
    "epochs=1000\n",
    "\n",
    "# Defining the learning rate\n",
    "eta=0.01 \n",
    "\n",
    "# Storing all weights, bias and errors for a visualisation\n",
    "w1List, w2List, bList, eList = [], [], [], []\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # Appending the current weight and error to the corresponding lists\n",
    "    w1List.append(w[1])\n",
    "    w2List.append(w[2])\n",
    "    bList.append(w[0])\n",
    "    eList.append(sum_of_costJ(w))\n",
    "    \n",
    "    # Minimizing the error (weight adaptation)\n",
    "    w = w + (-eta * slope_of_cost_function_at_w(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out the list of weights, bias and errors in steps of 50 epochs\n",
    "w1List[::50], w2List[::50], bList[::50], eList[::50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out the final weight\n",
    "print(f'Final weight of b (intercept): {w[0]}\\nFinal weight of x1: {w[1]}\\nFinal weight of x2: {w[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Plot the progression of the weights and the error (x-axis: weights, y-axis: errors).<br>\n",
    "<em>Hint: If you do not know how to make plots using the package matplotlib, get inspired by looking at the code for plots in this notebook and/or the official documentation.</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the progression of the weights, bias and the errors\n",
    "## TO-DO ##\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the new hyperplane to see how the classification has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the figure and axis\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Grid configuration (evenly spaced numbers over a specified interval)\n",
    "x1 = np.linspace(0,2.5,2)\n",
    "x2 = np.linspace(0,2.5,2)\n",
    "\n",
    "# Creation of the grid\n",
    "X1,X2 = np.meshgrid(x1,x2)\n",
    "\n",
    "# Plotting the initial hyperplane\n",
    "y_old = -0.5*X1 + 2*X2 + 1.5\n",
    "ax.plot_surface(X1, X2, y_old, color='#baffde', alpha=0.6)\n",
    "\n",
    "# Plotting the new hyperplane\n",
    "y_new = w[1]*X1 + w[2]*X2 + w[0]\n",
    "ax.plot_surface(X1, X2, y_new, color='#ab88cf', alpha=0.6)\n",
    "\n",
    "# Plotting the points (colours are the intended class)\n",
    "ax.scatter(0.5, -1.5, 1, c='#409fff', marker='o')\n",
    "ax.scatter(1, 0.5, 1, c='#409fff', marker='o')\n",
    "\n",
    "ax.scatter(-0.5, 0.4, -1, c='#f2305d', marker='^')\n",
    "\n",
    "plt.title(\"Initial Hyperplane (green) vs Adapted Hyperplane (purple)\")\n",
    "# due to a bug in the Poly3DCollection, it is currently not possible to add a legend for the hyperplanes\n",
    "ax.view_init(25, 30) # try changing these values to see how it rotates\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the threedimensional visualization can also become confusing, we can show that the adapted function works better than the original by rewriting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_adapted(p):\n",
    "    '''\n",
    "    This function returns the result of the adapted hyperplane of\n",
    "        y = 1.286473705185907*x1 + 0.6411130114825146 * x2 - 0.6099986551635744\n",
    "    '''\n",
    "    x1, x2, y = p\n",
    "    return w[1]*x1 + w[2]*x2 + w[0]\n",
    "\n",
    "result1, result2, result3 = f(p1), f(p2), f(p3)\n",
    "result1_adapted, result2_adapted, result3_adapted = f_adapted(p1), f_adapted(p2),f_adapted(p3)\n",
    "\n",
    "for org, adap in zip([result1, result2, result3], [result1_adapted, result2_adapted, result3_adapted]):\n",
    "    print(f'Original: {np.sign(org)}\\tAdapted: {np.sign(adap)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the classification is now correct for all points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting $y=0$, we can project the classifier into the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same points as above\n",
    "plt.scatter(2, -1.5, c='#409fff',marker='o')\n",
    "plt.scatter(1, 0.5, c='#409fff',marker='o')\n",
    "\n",
    "plt.scatter(-0.5, 0.4, c='#f2305d',marker='^')\n",
    "\n",
    "# Initial hyperplane\n",
    "plt.plot([4, -2], [0.25, -1.25], c='#baffde', linestyle = '-', label = 'original')\n",
    "\n",
    "# Adapted hyperplane\n",
    "plt.plot([4, -2], [-7.075033706, 4.964719181], c='#ab88cf', linestyle = '-', label='adapted')\n",
    "\n",
    "\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Linear equations and the gradient descent are also used for regression, not only classification. These two concepts use the same tools to achieve different goals.    \n",
    "Regression is the process of using features of items (independent/predictor variables) to predict a dependent variable. \n",
    "The dependent variable is what we call y, the independent variables are the different x<sub>1</sub>, x<sub>2</sub>, …, x<sub>n</sub>.   \n",
    "For example, we might want an equation that describes how many homophones a given word has. We could use <em>syllable complexity, word length, </em> and <em>word frequency</em> as our predictors (of course, we need to express them numerically).    \n",
    "The equation we are looking for accurately combines these variables with a certain weight per variable and yields a prediction for the number of homophones that a given word has.    \n",
    "\n",
    "$$\n",
    "w_1 \\cdot syllable\\;complexity + w_2 \\cdot word\\;length + w_3 \\cdot word\\;frequency + b = number\\;of\\;homophones\n",
    "$$\n",
    "\n",
    "## Example\n",
    "![image](figures/regression_line_mfcl.png)   \n",
    "\n",
    "In a plot that has been reduced to two dimensions, we cannot show all the different features that are responsible for predicting the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next steps we will use the function $f(x)=-0.5x + 1.5$ and the following points; $\\langle x=2, y=2\\rangle$, $\\langle x=1, y=0.4\\rangle$ and $\\langle x=2, y=0.5\\rangle$.\n",
    "\n",
    "We can predict our y-values ($y_1, y_2, y_3$) with the dot product or by doing the calculations explicitly (see code below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=[1,2] # absorbing the intercept (bias) into the feature vector x1 by setting 1 at index 0 (first dimension)\n",
    "x2=[1,1] # absorbing the intercept (bias) into the feature vector x2 by setting 1 at index 0 (first dimension)\n",
    "x3=[1,2] # absorbing the intercept (bias) into the feature vector x3 by setting 1 at index 0 (first dimension)\n",
    "\n",
    "y1=2\n",
    "y2=0.4\n",
    "y3=0.5\n",
    "\n",
    "# Defining the weights\n",
    "w=[1.5,-0.5] # absorbing the intercept (bias) into w at index 0 (first dimension)\n",
    "\n",
    "# Plotting the initial regression line\n",
    "plt.plot([0,3],[1.5,0], c='#6e6e6e', linestyle = '--')\n",
    "\n",
    "# Plotting the points\n",
    "plt.scatter([2,1,2],[2,0.4,0.5], color='#409fff')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Prediction of y1, y2 and y3 given x1, x2 and x3\n",
    "# Note the notation difference when calculating the prediction with the dot product or doing the calculations explicitly.\n",
    "print(\"Predicting y1 via dot product:\",np.dot(w,x1), \"or explicitly:\", x1[0]*w[0]+x1[1]*w[1])\n",
    "print(\"Predicting y2 via dot product:\",np.dot(w,x2), \"or explicitly:\", x2[0]*w[0]+x2[1]*w[1])\n",
    "print(\"Predicting y3 via dot product:\",np.dot(w,x3), \"or explicitly:\", x3[0]*w[0]+x3[1]*w[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "1. Plot point $(x_1, y_1)$ and its prediction.<br>\n",
    "2. What can be said about the relationship between the first $(x_1, y_1)$ and third $(x_3, y_3)$ point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the initial regression line\n",
    "## TO-DO ##\n",
    "\n",
    "# Plot of x1 and its prediction\n",
    "## TO-DO ##\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Question 4.2: ## TO-DO ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slope_of_cost_function_at_w(j):  \n",
    "    return -((y1-np.dot(w,x1))*x1[j]+    \n",
    "             (y2-np.dot(w,x2))*x2[j]+\n",
    "             (y3-np.dot(w,x3))*x3[j])  \n",
    "\n",
    "eta=0.01\n",
    "epochs=1000\n",
    "\n",
    "for i in range(epochs):\n",
    "    w[0]= w[0] + (-eta * slope_of_cost_function_at_w(0))\n",
    "    w[1]= w[1] + (-eta * slope_of_cost_function_at_w(1))\n",
    "    \n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learned function: f(x)=\",w[1],\"* x +\",w[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Once again plot the three points $\\langle x=2, y=2\\rangle$, $\\langle x=1, y=0.4\\rangle$ and $\\langle x=2, y=0.5\\rangle$ and the now updated regression line.<br>\n",
    "Tipp: If you do not know how to make plots using the package matplotlib, get inspired by looking at the code for plots in this notebook and the official documentation.\n",
    "1. What do you notice? How has the line changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the initial regression line\n",
    "## TO-DO ##\n",
    "\n",
    "# Plotting the new regression line\n",
    "## TO-DO ##\n",
    "\n",
    "# Plotting the points\n",
    "## TO-DO ##\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Question 5.1: ## TO-DO ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
